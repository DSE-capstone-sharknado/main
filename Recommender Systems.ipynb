{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommender Systems\n",
    "\n",
    "## User-Items Observation Matrix\n",
    "\n",
    "### Explicit Feedback\n",
    "\n",
    "Sparce matrix, e.g.: 133,960x431,826 = 57.8 Billion Positions where only 100M are not 0’s!\n",
    "$$\n",
    "\\mathbf{V}_1 \\times \\mathbf{V}_2 =  \\begin{vmatrix} \n",
    "ui & \\mathbf{i_1} & \\mathbf{i_2} \\\\\n",
    "\\mathbf{u_1} &  1 & 0 \\\\\n",
    "\\mathbf{u_2} &  1 & 0 \\\\\n",
    "\\end{vmatrix}\n",
    "$$\n",
    "\n",
    "### Implicit Feedback\n",
    "\n",
    "For this case the whole matrix is defined as interactions that did not happen are just 0's. However, we should have very little confidence in 0's b/c they don't imply anything, e.g.: maybe the user never had the chance to buy it — doesn't not imply dislike.\n",
    "\n",
    "## Ranking Function:\n",
    "\n",
    "$$\n",
    "r_{ui}\n",
    "$$\n",
    "\n",
    "Where *r* is the unobserved value by user *u* for item *i*. How do we find the *r* function?\n",
    "\n",
    "## Content-based Approach\n",
    "\n",
    "Get items user likes. Find other items w/ a low distance function.\n",
    "\n",
    "Pros: Finds similar items to past liked items (stuck in a bubble)\n",
    "\n",
    "Cons: Fails to survace orignal content\n",
    "\n",
    "## Collaborative Filtering\n",
    "\n",
    "Find users who liked what I liked. Then recommend what they liked (collaboration). Based upon the assumption that those who agreed in the past tend to agree in the future.\n",
    "\n",
    "### Memory Based - Neighborhood Models\n",
    "\n",
    "Find similar users (user-based CF) or items (item-based CF) to predict\tmissing ratings.\n",
    "\n",
    "Estimate rating by looking at simular items and running a weighted sum. First we must establish a simularity measure:\n",
    "$$\n",
    "S^k(i,u)\n",
    "$$\n",
    "Using the similarity measure, we identify the *k* items rated by *u*, which are most similar to *i*. This set of *k* neighbors is denoted by *S*. The predicted value of $r_{ui}$ is taken as a weighted average of the ratings for neighboring items:\n",
    "$$\n",
    "\\hat r_{ui} = \\frac{\\sum_{j\\in{S^k(i,u)}}s_{ij}r_{ui}}{\\sum_{j\\in{S^k(i,u)}}s_{ij}}\n",
    "$$\n",
    "All item-oriented models share a disadvantage in regards to implicit feedback --they do not provide the flexibility to make a distinction between user preferences and the confidence we might have in those preferences.\n",
    "\n",
    "Cons: Memory-based. Expensive online simularity computation — does not scale.\n",
    "\n",
    "### Model Based - Latent Factor Models\n",
    "\n",
    "Matrix Factorization (MF) methods relate users and itemsby uncovering latent dimensions such that users have similar representations to items they rate highly, and are thebasis of many state-of-the-art recommendation approaches(e.g. Rendle et al. (2009), Bell, Koren, and Volinsky (2007),Bennett and Lanning (2007)). \n",
    "\n",
    "Ratings are deeply influenced by a set of factors that are very specific to the domain (e.g. amount of action i movies, compleixty of characters). These factors are in genreal not obvious, we might be able to think of some of them but it's hard to estimate their impact on ratings. The goal is to infer those so called *latent factors* from teh rating data by using mathametical techniques. \t\t\n",
    "\n",
    "A typical model associates each user $u$ with a user-factors vector $x_u\\in{R^f}$ , and\n",
    "each item $i$ with an item-factors vector  $y_i\\in{R^f}$. The prediction is done by taking an inner product, i.e., $\\hat{r_{ui}}=x_u^Ty_i$. The more involved part is parameter estimation. Many of the recent works, applied to explicit feedback datasets, suggested modeling directly only the observed ratings, while avoiding overfitting through an adequate regularized model, such as:\n",
    "$$\n",
    "min_{xy} \\sum_{r_{u,i} \\text{is known}}(r_{ui}-x_u^Ty_i)^2 + \\lambda(|x_u|_2^2 + |y_i|_2^2)\n",
    "$$\n",
    "where the last term is the regularization term using the L2 norm of the two parameters we are interested in optimizing.​​\n",
    "\n",
    "#### Point-wise methods\n",
    "\n",
    "Are these considered loss functions? see: http://lyst.github.io/lightfm/docs/lightfm.html\n",
    "\n",
    "##### SVD MF\n",
    "\n",
    "What SVD does is that it decomposes the user-product matrix into features for each user and each product.\n",
    "\n",
    "When using SVD for Collaborative Filtering, the idea is that one has a data matrix *M* relating each **User** (rows) with potential **Items** (columns) that could be recommended.  The matrix *M* (often referred to as the *utility matrix*) can be filled in either with explicit feedback (stars) or implicit feedback (e.g. purchase counts).  What happens in the SVD Collaborative Filtering case is that the non-missing entries that are in *M* (e.g. the movies a user *has rated*) are used to “factor” *M* into three component matrices: \n",
    "\n",
    "1. U-- A matrix relating the users to detected latent features.\n",
    "2. V-- A matrix relating the items to these same latent features.\n",
    "3. S-- A diagonal matrix giving the relative significance of these latent features.\n",
    "\n",
    "These matrices can then be recombined through multiplication to “fill in” the missing values in the original utility matrix $M=USV^T$\n",
    "\n",
    "\n",
    "\n",
    "##### Weighted Regularized MF\n",
    "\n",
    "An adaption of a SVD, which minimizes the squared loss. Their extensions are regularization to prevent overfitting and weights in the error function to increase the impact of positive feedback. In total their optimization criterion is:\n",
    "$$\n",
    "min_{xy} \\sum_{r_{u,i} \\text{is known}}(r_{ui}-x_u^Ty_i)^2 + \\lambda(|x_u|_2^2 + |y_i|_2^2)\n",
    "$$\n",
    "The problem w/ this method is that it is inherintly sequental. ALS to solve the cost function is better suited for parallelization\n",
    "\n",
    "##### ALS-WR\n",
    "\n",
    "Solves the cost function in a way that is paralleizable \n",
    "\n",
    "#### Pair-wise methods\n",
    "\n",
    "##### MM-MF\n",
    "\n",
    "A pairwise MF model from Gantner et al.(2011), which is optimized for a hinge ranking loss onxuij and trained using SGA as in BPR-MF.\n",
    "\n",
    "##### Baysian Personalized Recommenadation  BRP MF\n",
    "\n",
    "Personalization is some type of thing in classification.\n",
    "\n",
    "Pairwise loss function. Maximises the prediction difference between a positive example and a randomly chosen negative example. Useful when only positive interactions are present and optimising ROC AUC is desired.\n",
    "\n",
    "​\t\n",
    "\n",
    "------\n",
    "\n",
    "## Appendix\n",
    "\n",
    "Good slides on latent factors/SVD on implicit sets http://www.slideshare.net/sscdotopen/latent-factor-models-for-collaborative-filtering\n",
    "\n",
    "\n",
    "\n",
    "#### Implicit Feedback Condierations\n",
    "\n",
    "We have to modify this model slightly to work with implicit dataset. The model will be more heavly weighted w/ 0s which hold no meaning so we need a way to offset that. One common appraoch is *weighted matrix factorization*. \n",
    "$$\n",
    "p_{ui}\n",
    "$$\n",
    "which equals 1 if r>0 or 0 if r=0.\n",
    "\n",
    "We then model confidence:\n",
    "$$\n",
    "c_{ui} = 1+\\alpha r_{ui}\n",
    "$$\n",
    "We now want to minimze the following cost function:\n",
    "$$\n",
    "min_{xy} \\sum_{r_{u,i} \\text{is known}}(r_{ui}-x_u^Ty_i)^2 + \\lambda(|x_u|_2^2 + |y_i|_2^2)\n",
    "$$\n",
    "\n",
    "####  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
